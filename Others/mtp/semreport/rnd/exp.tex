\chapter{Experiments}

Good summary of text document expected to have good coverage and non-redundancy (novelty).

Objective functions for extracive summarization usually measure these two components separately and combine them together with tradeoff between encouraging the relavency and penalizing for redundancy.\\


\cbox{Objective\\}{
$F(S) = L(S) + \lambda R(S)$\\
F(S) measures the coverage \\
R(S) rewards diversity\\
$\lambda > 0$ is a trade-off coefficent.
}

\section{Coverage Functions}

Coverage functions, measures the representativeness of summary sentences to the whole document,

The following functions measures coverage of each sentence in a text document by summary sentences.

$L(S) = \sum_{i \in V} C_i(S)$

One simple way to define $C_i(S)$
is just to use,

$C_i(S) = \sum_{j \in S} w_{i,j}$

where $w_{i,j} \geq 0$ measures the similarity between sentence i and j.

\cbox{Coverage Function\\}{
$L(S) = \sum_{i \in V} \sum_{j \in S} w_{i,j}$
}

\subsection{TFxIDF}

Here $w_{i,j}$ is cosine similarity between sentence i and sentecen j.

Documents were pre-processed by segmenting sentences and stemming words using the Porter Stemmer. Each sentence was represented using a bag-of-terms vector, where we used context terms up to bi-grams. Similarity between sentence i and sentence j, was computed using cosine similarity,\\

\cbox{Sentence Similarity (TFxIDF)\\}{
$w_{i,j} = \frac{\sum_{w in s_i} tf_{w,i} x tf_{w,j} x idf_w^2 }{ \sqrt{\sum_{w in s_i} tf_{w,i}^2 idf_i^2 } \sqrt{\sum_{w in s_j} tf_{w,j}^2 idf_j^2 } }$ 
}

where $tf_{w,i}$ and $tf_{w,j}$ are the numbers of times that w appears in $s_i$ and sentence $s_j$ respectively, and
$idf_{w}$ is the inverse document frequency (IDF) of term w, which was calculated as the
logarithm of the ratio of the number of articles that w appears over the total number of all articles in the
document cluster.



\subsection{Semantic Measures}

\section{Diversity or Reward Functions}
Instead of penalizing the redundancy by subtracting from the objective, rewarding diversity is used here.\\

\cbox{Diversity Function\\}{
$R(S) = \sum_{i=1}^{K}{\sqrt{\sum_{j \in P_i  \cap S }{r_j}}}$\\
$P_{i}$ is the set of sentences in Cluster i\\
$r_{j}$ is the reward for the sentence j
}

where $P_i$ is a partition of the ground set V into separate clusters. The value $r_i$ estimates the importance of i to
the summary. The function $R(S)$ rewards diversity in that there is usually more benefit to selecting a sentence from a cluster not yet having one of its
elements already chosen. After a sentence is selected from a cluster, other sentences from the same cluster start having diminishing gain, because of the square root function. 


\subsection{Clustering}

\begin{enumerate}
	\item K-means Clustering
	\item Single Link Clustering
\end{enumerate}

\section{Results}

\section{Summary}
In this chapter, we described the experiments done in unsupevised and supervised settings. First unsupervised approach is based on \textit{number of occurences and proximity}. Second unsupervised approach is based on \textit{ modified textrank which includes co-referencing for constructing a text graph}. 


In the next chapter,  we conclude this report with future plans.